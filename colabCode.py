# -*- coding: utf-8 -*-
"""YapayZekaFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmEGvBiOkd-U5blowdUNHXd6KBdo44Y1

# **BÃ¶lÃ¼m 1 â€“ Veri Setlerinin YÃ¼klenmesi**
"""

import pandas as pd

books = pd.read_csv("books.csv")
ratings = pd.read_csv("ratings.csv")
tags = pd.read_csv("tags.csv")
book_tags = pd.read_csv("book_tags.csv")
to_read = pd.read_csv("to_read.csv")

"""# **BÃ¶lÃ¼m 2 â€“ Veri Setlerini TanÄ±ma**"""

# Ä°lk 5 satÄ±r
print("ğŸ“˜ books.csv")
print(books.head())

print("\nâ­ ratings.csv")
print(ratings.head())

print("\nğŸ”– tags.csv")
print(tags.head())

print("\nğŸ”— book_tags.csv")
print(book_tags.head())

print("\nğŸ“š to_read.csv")
print(to_read.head())

# YapÄ±sal bilgi (sÃ¼tunlar, veri tipi, boÅŸluklar)
books.info()
ratings.info()
tags.info()
book_tags.info()
to_read.info()

"""# **BÃ¶lÃ¼m 3 â€“ Eksik Veri Analizi**"""

def eksik_veri_raporu(df, ad):
    print(f"\n{ad} veri seti:")
    nulls = df.isnull().sum()
    nulls = nulls[nulls > 0]
    if nulls.empty:
        print("â†’ Eksik veri yok.")
    else:
        print(nulls)

eksik_veri_raporu(books, "books.csv")
eksik_veri_raporu(ratings, "ratings.csv")
eksik_veri_raporu(tags, "tags.csv")
eksik_veri_raporu(book_tags, "book_tags.csv")
eksik_veri_raporu(to_read, "to_read.csv")

"""# **BÃ¶lÃ¼m 4 â€“ AykÄ±rÄ± DeÄŸer Analizi**

Bu adÄ±mda veri setimizdeki uÃ§ kullanÄ±cÄ±larÄ± ve kitaplarÄ± analiz edeceÄŸiz. Hedefimiz:

Ã‡ok az veya Ã§ok fazla puan verilmiÅŸ kitaplarÄ± bulmak

AÅŸÄ±rÄ± derecede puanlama yapan kullanÄ±cÄ±larÄ± tespit etmek (bot olabilir)

Ortalama puan daÄŸÄ±lÄ±mÄ±nÄ± incelemek
"""

import matplotlib.pyplot as plt

# Her kitap kaÃ§ kez puanlanmÄ±ÅŸ?
book_rating_counts = ratings['book_id'].value_counts()

# Ä°statistiksel Ã¶zet
print("Kitap baÅŸÄ±na verilen puan istatistikleri:")
print(book_rating_counts.describe())

# GÃ¶rselleÅŸtirme
plt.figure(figsize=(10,5))
plt.hist(book_rating_counts, bins=50, color='skyblue')
plt.xlabel("Bir kitabÄ±n kaÃ§ kez puanlandÄ±ÄŸÄ±")
plt.ylabel("Kitap sayÄ±sÄ±")
plt.title("Kitaplara Verilen Puan DaÄŸÄ±lÄ±mÄ±")
plt.grid(True)
plt.show()

"""KitaplarÄ±n Puanlanma YoÄŸunluÄŸu"""

# Her kullanÄ±cÄ± kaÃ§ kitap puanlamÄ±ÅŸ?
user_rating_counts = ratings['user_id'].value_counts()

print("KullanÄ±cÄ± baÅŸÄ±na puan sayÄ±sÄ± istatistikleri:")
print(user_rating_counts.describe())

# GÃ¶rselleÅŸtirme
plt.figure(figsize=(10,5))
plt.hist(user_rating_counts, bins=50, color='salmon')
plt.xlabel("Bir kullanÄ±cÄ±nÄ±n kaÃ§ kitap puanladÄ±ÄŸÄ±")
plt.ylabel("KullanÄ±cÄ± sayÄ±sÄ±")
plt.title("KullanÄ±cÄ±larÄ±n Puanlama DaÄŸÄ±lÄ±mÄ±")
plt.grid(True)
plt.show()

"""KullanÄ±cÄ±larÄ±n Puanlama DavranÄ±ÅŸlarÄ±"""

# 10'dan az kitap puanlayan kullanÄ±cÄ±lar
rare_users = user_rating_counts[user_rating_counts < 10]
print(f"10'dan az kitap puanlayan kullanÄ±cÄ± sayÄ±sÄ±: {len(rare_users)}")

# 5'ten az puan almÄ±ÅŸ kitaplar
rare_books = book_rating_counts[book_rating_counts < 5]
print(f"5'ten az kez puanlanan kitap sayÄ±sÄ±: {len(rare_books)}")

"""AykÄ±rÄ± KullanÄ±cÄ± ve Kitap EÅŸikleri Belirleme

# **BÃ¶lÃ¼m 5 â€“ Veri TemizliÄŸi ve Filtreleme**
"""

# KitaplarÄ±n kaÃ§ kez puanlandÄ±ÄŸÄ±nÄ± tekrar alalÄ±m
book_rating_counts = ratings['book_id'].value_counts()

# Belirli bir eÅŸiÄŸin Ã¼zerindekileri filtreleyelim
aykiri_esik = 4000
popular_books = book_rating_counts[book_rating_counts < aykiri_esik].index

# Bu eÅŸik altÄ±ndaki kitaplarÄ± iÃ§eren yeni ratings tablosu oluÅŸturalÄ±m
filtered_ratings = ratings[ratings['book_id'].isin(popular_books)]

print(f"FiltrelenmiÅŸ rating sayÄ±sÄ±: {filtered_ratings.shape[0]}")
print(f"Orijinal rating sayÄ±sÄ±: {ratings.shape[0]}")

"""AÅŸÄ±rÄ± PuanlanmÄ±ÅŸ KitaplarÄ± KaldÄ±rmak

# **BÃ¶lÃ¼m 6 â€“ Ä°Ã§erik TabanlÄ± Ã–neri: TF-IDF ve Benzerlik HesabÄ±**

Hedefimiz:
tag_name sÃ¼tunundaki verileri kullanarak TF-IDF vektÃ¶rleri oluÅŸturmak

Kitaplar arasÄ± cosine similarity matrisini hesaplamak

AÃ§Ä±klama: TF-IDF Nedir?
TF-IDF (Term Frequency â€“ Inverse Document Frequency), her kelimenin bir belge (bizim iÃ§in kitap) iÃ§indeki Ã¶nemini belirlemeye yarayan bir tekniktir.

Etiketleri bu yaklaÅŸÄ±mla sayÄ±sallaÅŸtÄ±rÄ±rsak, kitaplarÄ± vektÃ¶rler halinde karÅŸÄ±laÅŸtÄ±rabiliriz.

Gerekli kÃ¼tÃ¼phane:
TfidfVectorizer (scikit-learn'den)
"""

# Gerekli kÃ¼tÃ¼phaneler
import pandas as pd

# CSV dosyalarÄ±nÄ± oku
books = pd.read_csv("books.csv")
tags = pd.read_csv("tags.csv")
book_tags = pd.read_csv("book_tags.csv")

# 1. Etiket isimlerini tag_id ile eÅŸleÅŸtir
book_tags_merged = pd.merge(book_tags, tags, on="tag_id", how="left")

# 2. Her kitap iÃ§in tÃ¼m etiketleri birleÅŸtir
book_tags_grouped = book_tags_merged.groupby(
    "goodreads_book_id")["tag_name"].apply(lambda x: ' '.join(x)).reset_index()

# 3. Etiketleri kitap bilgileriyle birleÅŸtir
books_with_tags = pd.merge(
    books, book_tags_grouped, on="goodreads_book_id", how="left")

# Kontrol
books_with_tags[['book_id', 'title', 'tag_name']].head()

"""Etiketleri Kitaplara BirleÅŸtir"""

# Gerekli kÃ¼tÃ¼phane
from sklearn.feature_extraction.text import TfidfVectorizer

# NaN deÄŸerleri boÅŸ string'e Ã§eviriyoruz
books_with_tags['tag_name'] = books_with_tags['tag_name'].fillna("")

# TF-IDF vektÃ¶rizer nesnesi
tfidf = TfidfVectorizer(stop_words='english')  # Ä°ngilizce stop word'leri Ã§Ä±kar

# Kitap etiketlerinden TF-IDF matrisi oluÅŸtur
tfidf_matrix = tfidf.fit_transform(books_with_tags['tag_name'])

# Matris boyutunu kontrol et
print("TF-IDF matrisi oluÅŸturuldu!")
print(f"Boyut: {tfidf_matrix.shape[0]} kitap x {tfidf_matrix.shape[1]} kelime")

"""

```
# Bu, kod olarak biÃ§imlendirilmiÅŸtir
```

TF-IDF VektÃ¶rlerinin OluÅŸturulmasÄ±"""

from sklearn.metrics.pairwise import cosine_similarity

# Kitaplar arasÄ± benzerlik matrisi
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Matris boyutu (10000 x 10000) olacak
print("Cosine similarity matrisi oluÅŸturuldu.")
print(f"Boyut: {cosine_sim.shape}")

"""Kitaplar ArasÄ± Benzerlik: Cosine Similarity
Hedef:
TF-IDF matrisini kullanarak her kitabÄ±n diÄŸer kitaplarla benzerlik oranÄ±nÄ± hesaplayacaÄŸÄ±z.
"""

def kitap_Ã¶ner(kitap_adi, cosine_sim=cosine_sim, df=books_with_tags, top_n=10):
    # Kitap adÄ±nÄ±n indeksini bul
    indices = pd.Series(df.index, index=df['title'].str.lower())

    kitap_adi = kitap_adi.lower()
    if kitap_adi not in indices:
        print("Kitap bulunamadÄ±.")
        return

    idx = indices[kitap_adi]

    # Benzerlikleri sÄ±rala
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # Kendisi hariÃ§ en benzerleri al

    kitap_indices = [i[0] for i in sim_scores]

    # Kitap bilgileri + etiketlerle gÃ¶ster
    return df[['title', 'authors', 'tag_name']].iloc[kitap_indices]

"""Kitap AdÄ±na GÃ¶re Benzer Kitap Ã–ner
AmaÃ§:
KullanÄ±cÄ±nÄ±n girdiÄŸi bir kitap adÄ±na gÃ¶re, en benzer kitaplarÄ± bulmak.

SonuÃ§larÄ± baÅŸlÄ±k, yazar bilgisi ve etiketleriyle gÃ¶stermek
"""

kitap_Ã¶ner("To Kill a Mockingbird")

"""BÃ¶lÃ¼m 7 â€“ Ä°ÅŸbirlikÃ§i Filtreleme (User-Based Collaborative Filtering)**kalÄ±n metin**

# **BÃ¶lÃ¼m 7 â€“ Ä°ÅŸbirlikÃ§i Filtreleme (User-Based Collaborative Filtering)  (Pandas + sklearn)**

Bu yÃ¶ntemde Ã¶neriler, kullanÄ±cÄ±nÄ±n kitap puanlarÄ± ile benzer zevklere sahip diÄŸer kullanÄ±cÄ±larÄ±n puanlarÄ± karÅŸÄ±laÅŸtÄ±rÄ±larak yapÄ±lÄ±r.

Ã–rnek:

Sen Kitap A ve Bâ€™yi beÄŸendin. BaÅŸka biri de A, B ve Câ€™yi beÄŸendiyse â†’ sistem sana Kitap Câ€™yi Ã¶nerir.
"""

import pandas as pd

# filtered_ratings zaten aykÄ±rÄ± deÄŸer temizliÄŸi sonrasÄ± ratingsâ€™i iÃ§eriyor
df = filtered_ratings.copy()

# Pivot: satÄ±rlar user_id, sÃ¼tunlar book_id, deÄŸerler rating
user_item_matrix = df.pivot(index='user_id', columns='book_id', values='rating').fillna(0)

# HÄ±zlÄ± kontrol
print(user_item_matrix.shape)
user_item_matrix.head()

"""Veri HazÄ±rlÄ±ÄŸÄ±

filtered_ratings iÃ§inden kullanÄ±cÄ±-kitap matrisini oluÅŸturacaÄŸÄ±z (pivot).

Eksik deÄŸerleri 0 (yorum/puan yok) ile dolduracaÄŸÄ±z.
"""

from sklearn.neighbors import NearestNeighbors
import numpy as np

# Modeli tanÄ±mla
knn = NearestNeighbors(metric='cosine', algorithm='brute')
knn.fit(user_item_matrix.values)

# user_ids serisi (indexâ€™imiz)
users = list(user_item_matrix.index)

"""Model Kurulumu (NearestNeighbors ile)

Burada her kullanÄ±cÄ±yÄ± bir vektÃ¶r olarak alÄ±p, cosine benzerliÄŸiyle k-en yakÄ±n komÅŸusunu bulacaÄŸÄ±z:
"""

def user_based_recommend(user_id, k=5, n_recs=10):
    # 1. KullanÄ±cÄ± vektÃ¶rÃ¼nÃ¼ al
    user_idx = users.index(user_id)
    user_vec = user_item_matrix.values[user_idx].reshape(1, -1)

    # 2. En yakÄ±n k komÅŸuyu bul
    distances, indices = knn.kneighbors(user_vec, n_neighbors=k+1)
    # indices[0][0] kendisi, onu atlÄ±yoruz
    neighbor_idxs = indices[0][1:]

    # 3. KomÅŸu kullanÄ±cÄ±larÄ±n puanlarÄ±
    neighbor_ratings = user_item_matrix.values[neighbor_idxs]

    # 4. OrtalamalarÄ±nÄ± al (eÅŸ aÄŸÄ±rlÄ±klÄ±)
    mean_ratings = np.mean(neighbor_ratings, axis=0)

    # 5. KullanÄ±cÄ±nÄ±n zaten puan verdiÄŸi kitaplarÄ± sÄ±fÄ±rla
    user_rated = user_item_matrix.values[user_idx] > 0
    mean_ratings[user_rated] = 0

    # 6. En yÃ¼ksek n_recs kitap indisini al
    rec_idxs = np.argsort(mean_ratings)[::-1][:n_recs]
    rec_book_ids = user_item_matrix.columns[rec_idxs]

    # 7. Kitap baÅŸlÄ±klarÄ±yla eÅŸleÅŸtir
    return books.loc[books['book_id'].isin(rec_book_ids), ['book_id','title','authors']].drop_duplicates().head(n_recs)

# Ã–rnek: user_id = 15 iÃ§in 10 Ã¶neri
print(user_based_recommend(15, k=5, n_recs=10))

"""Ã–neri Fonksiyonu

â€œBir kullanÄ±cÄ±â€ iÃ§in:

O kullanÄ±cÄ±ya en yakÄ±n k kullanÄ±cÄ±yÄ± bul.

Bu kullanÄ±cÄ±larÄ±n yÃ¼ksek puan verdikleri kitaplardan, orijinal kullanÄ±cÄ±ya henÃ¼z puan vermediÄŸi kitaplarÄ± Ã¶ner.

# **BÃ¶lÃ¼m 7 â€“ Part 2: SeÃ§ilen Kitaplara GÃ¶re KullanÄ±cÄ± TabanlÄ± Ã–neri (ArayÃ¼z Uyumlu)**

Hedef:

KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi 1 veya daha fazla kitap ID'sine gÃ¶re,
bu kitaplara yÃ¼ksek puan vermiÅŸ kullanÄ±cÄ±larÄ± bulmak.

Girdi: kitap ID listesi
Ã‡Ä±ktÄ±: kitaplarÄ± beÄŸenen kullanÄ±cÄ±larÄ±n listesi
"""

def kitap_isimden_id_al(secimler, kitaplar_df):
    kitaplar_df['title_lower'] = kitaplar_df['title'].str.lower()
    secimler = [s.lower() for s in secimler]
    eslesen_ids = []
    for secim in secimler:
        bulunan = kitaplar_df[kitaplar_df['title_lower'].str.contains(secim)]['book_id'].unique().tolist()
        eslesen_ids.extend(bulunan)
    return list(set(eslesen_ids))

"""Kitap AdÄ±ndan IDâ€™ye DÃ¶nÃ¼ÅŸtÃ¼rme"""

secili_kitaplar = ["God's Smuggler", "Bear Snores On"]
book_ids = kitap_isimden_id_al(secili_kitaplar, books)
print(book_ids)

def kitap_oner_secim_uzerinden(
    book_ids, ratings_df, books_df, top_n=10, min_rating=4):
    """
    SeÃ§ilen kitaplara gÃ¶re Ã¶neri Ã¼retir. Etiket bilgilerini de dÃ¶ndÃ¼rÃ¼r.
    """
    # 1. Bu kitaplarÄ± beÄŸenen kullanÄ±cÄ±larÄ± al
    begenenler = ratings_df[
        (ratings_df['book_id'].isin(book_ids)) &
        (ratings_df['rating'] >= min_rating)
    ]['user_id'].unique()

    # 2. Bu kullanÄ±cÄ±larÄ±n diÄŸer yÃ¼ksek puanladÄ±ÄŸÄ± kitaplarÄ± al
    diger_kitaplar = ratings_df[
        (ratings_df['user_id'].isin(begenenler)) &
        (~ratings_df['book_id'].isin(book_ids)) & #KullanÄ±cÄ±nÄ±n seÃ§tikleri hariÃ§
        (ratings_df['rating'] >= min_rating)
    ]

    # 3. Kitaplara gÃ¶re Ã¶neri sÄ±ralamasÄ±
    kitap_onerileri = diger_kitaplar['book_id'].value_counts().head(
        top_n).index.tolist()

    # 4. Kitap bilgileri (etiketler dahil)
    sonuc = books_with_tags[books_with_tags['book_id'].isin(kitap_onerileri)][[
        'book_id', 'title', 'authors', 'tag_name']].drop_duplicates()

    return sonuc.head(top_n)

"""KullanÄ±cÄ±nÄ±n SeÃ§mediÄŸi, BeÄŸenilen DiÄŸer KitaplarÄ± Ã–ner"""

secili_kitaplar = ["God's Smuggler", "Bear Snores On"]
book_ids = kitap_isimden_id_al(secili_kitaplar, books)
kitap_oner_secim_uzerinden(book_ids, filtered_ratings, books)

"""# **BÃ¶lÃ¼m 9 â€“ SVD Modeli EÄŸitimi ve Kaydedilmesi (.pkl DosyasÄ±)**"""

!pip install numpy==1.24.3
import os
os.kill(os.getpid(), 9)  # Runtime'Ä± yeniden baÅŸlatÄ±r

pip install scikit-surprise

import pandas as pd
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
import joblib

# ratings verisini oku ve aykÄ±rÄ± kitaplarÄ± filtrele
ratings = pd.read_csv("ratings.csv")
book_rating_counts = ratings['book_id'].value_counts()
popular_books = book_rating_counts[book_rating_counts < 5000].index
filtered_ratings = ratings[ratings['book_id'].isin(popular_books)]

# Surprise veri formatÄ±
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(filtered_ratings[
    ['user_id', 'book_id', 'rating']], reader)

# EÄŸitim-test bÃ¶l
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# SVD modeli eÄŸit
svd_model = SVD()
svd_model.fit(trainset)

# Kaydet
joblib.dump(svd_model, "svd_model.pkl")
print("âœ… SVD modeli baÅŸarÄ±yla kaydedildi.")

from surprise import accuracy

# 1. Veriyi ayÄ±r
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# 2. EÄŸit
svd_model = SVD()
svd_model.fit(trainset)

# 3. Test et
predictions = svd_model.test(testset)

# 4. RMSE ve MAE hesapla
print("RMSE:", accuracy.rmse(predictions))
print("MAE :", accuracy.mae(predictions))

dogru_tahmin_sayisi = sum(abs(p.r_ui - p.est) <= 1.0 for p in predictions)
dogruluk_orani = dogru_tahmin_sayisi / len(predictions)
print(f"DoÄŸruluk oranÄ±: {dogruluk_orani:.2%}")

"""# **BÃ¶lÃ¼m 8 â€“ Streamlit ArayÃ¼zÃ¼ ile Ã–neri UygulamasÄ±**

Hedefimiz:

KullanÄ±cÄ±nÄ±n kitap adÄ± seÃ§tiÄŸi ve Ã¶neri sonuÃ§larÄ±nÄ± tablo halinde gÃ¶rdÃ¼ÄŸÃ¼ basit ama iÅŸlevsel bir arayÃ¼z kurmak.

Ne KullanacaÄŸÄ±z?

*   streamlit (arayÃ¼z)
*   books.csv, filtered_ratings
*   books_with_tags (etiketli kitap bilgisi)

FonksiyonlarÄ±mÄ±z:
*   kitap_isimden_id_al()
*   kitap_oner_secim_uzerinden()
"""

pip install streamlit