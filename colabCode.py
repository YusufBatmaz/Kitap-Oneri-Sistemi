# -*- coding: utf-8 -*-
"""YapayZekaFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmEGvBiOkd-U5blowdUNHXd6KBdo44Y1

# **Bölüm 1 – Veri Setlerinin Yüklenmesi**
"""

import pandas as pd

books = pd.read_csv("books.csv")
ratings = pd.read_csv("ratings.csv")
tags = pd.read_csv("tags.csv")
book_tags = pd.read_csv("book_tags.csv")
to_read = pd.read_csv("to_read.csv")

"""# **Bölüm 2 – Veri Setlerini Tanıma**"""

# İlk 5 satır
print("📘 books.csv")
print(books.head())

print("\n⭐ ratings.csv")
print(ratings.head())

print("\n🔖 tags.csv")
print(tags.head())

print("\n🔗 book_tags.csv")
print(book_tags.head())

print("\n📚 to_read.csv")
print(to_read.head())

# Yapısal bilgi (sütunlar, veri tipi, boşluklar)
books.info()
ratings.info()
tags.info()
book_tags.info()
to_read.info()

"""# **Bölüm 3 – Eksik Veri Analizi**"""

def eksik_veri_raporu(df, ad):
    print(f"\n{ad} veri seti:")
    nulls = df.isnull().sum()
    nulls = nulls[nulls > 0]
    if nulls.empty:
        print("→ Eksik veri yok.")
    else:
        print(nulls)

eksik_veri_raporu(books, "books.csv")
eksik_veri_raporu(ratings, "ratings.csv")
eksik_veri_raporu(tags, "tags.csv")
eksik_veri_raporu(book_tags, "book_tags.csv")
eksik_veri_raporu(to_read, "to_read.csv")

"""# **Bölüm 4 – Aykırı Değer Analizi**

Bu adımda veri setimizdeki uç kullanıcıları ve kitapları analiz edeceğiz. Hedefimiz:

Çok az veya çok fazla puan verilmiş kitapları bulmak

Aşırı derecede puanlama yapan kullanıcıları tespit etmek (bot olabilir)

Ortalama puan dağılımını incelemek
"""

import matplotlib.pyplot as plt

# Her kitap kaç kez puanlanmış?
book_rating_counts = ratings['book_id'].value_counts()

# İstatistiksel özet
print("Kitap başına verilen puan istatistikleri:")
print(book_rating_counts.describe())

# Görselleştirme
plt.figure(figsize=(10,5))
plt.hist(book_rating_counts, bins=50, color='skyblue')
plt.xlabel("Bir kitabın kaç kez puanlandığı")
plt.ylabel("Kitap sayısı")
plt.title("Kitaplara Verilen Puan Dağılımı")
plt.grid(True)
plt.show()

"""Kitapların Puanlanma Yoğunluğu"""

# Her kullanıcı kaç kitap puanlamış?
user_rating_counts = ratings['user_id'].value_counts()

print("Kullanıcı başına puan sayısı istatistikleri:")
print(user_rating_counts.describe())

# Görselleştirme
plt.figure(figsize=(10,5))
plt.hist(user_rating_counts, bins=50, color='salmon')
plt.xlabel("Bir kullanıcının kaç kitap puanladığı")
plt.ylabel("Kullanıcı sayısı")
plt.title("Kullanıcıların Puanlama Dağılımı")
plt.grid(True)
plt.show()

"""Kullanıcıların Puanlama Davranışları"""

# 10'dan az kitap puanlayan kullanıcılar
rare_users = user_rating_counts[user_rating_counts < 10]
print(f"10'dan az kitap puanlayan kullanıcı sayısı: {len(rare_users)}")

# 5'ten az puan almış kitaplar
rare_books = book_rating_counts[book_rating_counts < 5]
print(f"5'ten az kez puanlanan kitap sayısı: {len(rare_books)}")

"""Aykırı Kullanıcı ve Kitap Eşikleri Belirleme

# **Bölüm 5 – Veri Temizliği ve Filtreleme**
"""

# Kitapların kaç kez puanlandığını tekrar alalım
book_rating_counts = ratings['book_id'].value_counts()

# Belirli bir eşiğin üzerindekileri filtreleyelim
aykiri_esik = 4000
popular_books = book_rating_counts[book_rating_counts < aykiri_esik].index

# Bu eşik altındaki kitapları içeren yeni ratings tablosu oluşturalım
filtered_ratings = ratings[ratings['book_id'].isin(popular_books)]

print(f"Filtrelenmiş rating sayısı: {filtered_ratings.shape[0]}")
print(f"Orijinal rating sayısı: {ratings.shape[0]}")

"""Aşırı Puanlanmış Kitapları Kaldırmak

# **Bölüm 6 – İçerik Tabanlı Öneri: TF-IDF ve Benzerlik Hesabı**

Hedefimiz:
tag_name sütunundaki verileri kullanarak TF-IDF vektörleri oluşturmak

Kitaplar arası cosine similarity matrisini hesaplamak

Açıklama: TF-IDF Nedir?
TF-IDF (Term Frequency – Inverse Document Frequency), her kelimenin bir belge (bizim için kitap) içindeki önemini belirlemeye yarayan bir tekniktir.

Etiketleri bu yaklaşımla sayısallaştırırsak, kitapları vektörler halinde karşılaştırabiliriz.

Gerekli kütüphane:
TfidfVectorizer (scikit-learn'den)
"""

# Gerekli kütüphaneler
import pandas as pd

# CSV dosyalarını oku
books = pd.read_csv("books.csv")
tags = pd.read_csv("tags.csv")
book_tags = pd.read_csv("book_tags.csv")

# 1. Etiket isimlerini tag_id ile eşleştir
book_tags_merged = pd.merge(book_tags, tags, on="tag_id", how="left")

# 2. Her kitap için tüm etiketleri birleştir
book_tags_grouped = book_tags_merged.groupby(
    "goodreads_book_id")["tag_name"].apply(lambda x: ' '.join(x)).reset_index()

# 3. Etiketleri kitap bilgileriyle birleştir
books_with_tags = pd.merge(
    books, book_tags_grouped, on="goodreads_book_id", how="left")

# Kontrol
books_with_tags[['book_id', 'title', 'tag_name']].head()

"""Etiketleri Kitaplara Birleştir"""

# Gerekli kütüphane
from sklearn.feature_extraction.text import TfidfVectorizer

# NaN değerleri boş string'e çeviriyoruz
books_with_tags['tag_name'] = books_with_tags['tag_name'].fillna("")

# TF-IDF vektörizer nesnesi
tfidf = TfidfVectorizer(stop_words='english')  # İngilizce stop word'leri çıkar

# Kitap etiketlerinden TF-IDF matrisi oluştur
tfidf_matrix = tfidf.fit_transform(books_with_tags['tag_name'])

# Matris boyutunu kontrol et
print("TF-IDF matrisi oluşturuldu!")
print(f"Boyut: {tfidf_matrix.shape[0]} kitap x {tfidf_matrix.shape[1]} kelime")

"""

```
# Bu, kod olarak biçimlendirilmiştir
```

TF-IDF Vektörlerinin Oluşturulması"""

from sklearn.metrics.pairwise import cosine_similarity

# Kitaplar arası benzerlik matrisi
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Matris boyutu (10000 x 10000) olacak
print("Cosine similarity matrisi oluşturuldu.")
print(f"Boyut: {cosine_sim.shape}")

"""Kitaplar Arası Benzerlik: Cosine Similarity
Hedef:
TF-IDF matrisini kullanarak her kitabın diğer kitaplarla benzerlik oranını hesaplayacağız.
"""

def kitap_öner(kitap_adi, cosine_sim=cosine_sim, df=books_with_tags, top_n=10):
    # Kitap adının indeksini bul
    indices = pd.Series(df.index, index=df['title'].str.lower())

    kitap_adi = kitap_adi.lower()
    if kitap_adi not in indices:
        print("Kitap bulunamadı.")
        return

    idx = indices[kitap_adi]

    # Benzerlikleri sırala
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # Kendisi hariç en benzerleri al

    kitap_indices = [i[0] for i in sim_scores]

    # Kitap bilgileri + etiketlerle göster
    return df[['title', 'authors', 'tag_name']].iloc[kitap_indices]

"""Kitap Adına Göre Benzer Kitap Öner
Amaç:
Kullanıcının girdiği bir kitap adına göre, en benzer kitapları bulmak.

Sonuçları başlık, yazar bilgisi ve etiketleriyle göstermek
"""

kitap_öner("To Kill a Mockingbird")

"""Bölüm 7 – İşbirlikçi Filtreleme (User-Based Collaborative Filtering)**kalın metin**

# **Bölüm 7 – İşbirlikçi Filtreleme (User-Based Collaborative Filtering)  (Pandas + sklearn)**

Bu yöntemde öneriler, kullanıcının kitap puanları ile benzer zevklere sahip diğer kullanıcıların puanları karşılaştırılarak yapılır.

Örnek:

Sen Kitap A ve B’yi beğendin. Başka biri de A, B ve C’yi beğendiyse → sistem sana Kitap C’yi önerir.
"""

import pandas as pd

# filtered_ratings zaten aykırı değer temizliği sonrası ratings’i içeriyor
df = filtered_ratings.copy()

# Pivot: satırlar user_id, sütunlar book_id, değerler rating
user_item_matrix = df.pivot(index='user_id', columns='book_id', values='rating').fillna(0)

# Hızlı kontrol
print(user_item_matrix.shape)
user_item_matrix.head()

"""Veri Hazırlığı

filtered_ratings içinden kullanıcı-kitap matrisini oluşturacağız (pivot).

Eksik değerleri 0 (yorum/puan yok) ile dolduracağız.
"""

from sklearn.neighbors import NearestNeighbors
import numpy as np

# Modeli tanımla
knn = NearestNeighbors(metric='cosine', algorithm='brute')
knn.fit(user_item_matrix.values)

# user_ids serisi (index’imiz)
users = list(user_item_matrix.index)

"""Model Kurulumu (NearestNeighbors ile)

Burada her kullanıcıyı bir vektör olarak alıp, cosine benzerliğiyle k-en yakın komşusunu bulacağız:
"""

def user_based_recommend(user_id, k=5, n_recs=10):
    # 1. Kullanıcı vektörünü al
    user_idx = users.index(user_id)
    user_vec = user_item_matrix.values[user_idx].reshape(1, -1)

    # 2. En yakın k komşuyu bul
    distances, indices = knn.kneighbors(user_vec, n_neighbors=k+1)
    # indices[0][0] kendisi, onu atlıyoruz
    neighbor_idxs = indices[0][1:]

    # 3. Komşu kullanıcıların puanları
    neighbor_ratings = user_item_matrix.values[neighbor_idxs]

    # 4. Ortalamalarını al (eş ağırlıklı)
    mean_ratings = np.mean(neighbor_ratings, axis=0)

    # 5. Kullanıcının zaten puan verdiği kitapları sıfırla
    user_rated = user_item_matrix.values[user_idx] > 0
    mean_ratings[user_rated] = 0

    # 6. En yüksek n_recs kitap indisini al
    rec_idxs = np.argsort(mean_ratings)[::-1][:n_recs]
    rec_book_ids = user_item_matrix.columns[rec_idxs]

    # 7. Kitap başlıklarıyla eşleştir
    return books.loc[books['book_id'].isin(rec_book_ids), ['book_id','title','authors']].drop_duplicates().head(n_recs)

# Örnek: user_id = 15 için 10 öneri
print(user_based_recommend(15, k=5, n_recs=10))

"""Öneri Fonksiyonu

“Bir kullanıcı” için:

O kullanıcıya en yakın k kullanıcıyı bul.

Bu kullanıcıların yüksek puan verdikleri kitaplardan, orijinal kullanıcıya henüz puan vermediği kitapları öner.

# **Bölüm 7 – Part 2: Seçilen Kitaplara Göre Kullanıcı Tabanlı Öneri (Arayüz Uyumlu)**

Hedef:

Kullanıcının seçtiği 1 veya daha fazla kitap ID'sine göre,
bu kitaplara yüksek puan vermiş kullanıcıları bulmak.

Girdi: kitap ID listesi
Çıktı: kitapları beğenen kullanıcıların listesi
"""

def kitap_isimden_id_al(secimler, kitaplar_df):
    kitaplar_df['title_lower'] = kitaplar_df['title'].str.lower()
    secimler = [s.lower() for s in secimler]
    eslesen_ids = []
    for secim in secimler:
        bulunan = kitaplar_df[kitaplar_df['title_lower'].str.contains(secim)]['book_id'].unique().tolist()
        eslesen_ids.extend(bulunan)
    return list(set(eslesen_ids))

"""Kitap Adından ID’ye Dönüştürme"""

secili_kitaplar = ["God's Smuggler", "Bear Snores On"]
book_ids = kitap_isimden_id_al(secili_kitaplar, books)
print(book_ids)

def kitap_oner_secim_uzerinden(
    book_ids, ratings_df, books_df, top_n=10, min_rating=4):
    """
    Seçilen kitaplara göre öneri üretir. Etiket bilgilerini de döndürür.
    """
    # 1. Bu kitapları beğenen kullanıcıları al
    begenenler = ratings_df[
        (ratings_df['book_id'].isin(book_ids)) &
        (ratings_df['rating'] >= min_rating)
    ]['user_id'].unique()

    # 2. Bu kullanıcıların diğer yüksek puanladığı kitapları al
    diger_kitaplar = ratings_df[
        (ratings_df['user_id'].isin(begenenler)) &
        (~ratings_df['book_id'].isin(book_ids)) & #Kullanıcının seçtikleri hariç
        (ratings_df['rating'] >= min_rating)
    ]

    # 3. Kitaplara göre öneri sıralaması
    kitap_onerileri = diger_kitaplar['book_id'].value_counts().head(
        top_n).index.tolist()

    # 4. Kitap bilgileri (etiketler dahil)
    sonuc = books_with_tags[books_with_tags['book_id'].isin(kitap_onerileri)][[
        'book_id', 'title', 'authors', 'tag_name']].drop_duplicates()

    return sonuc.head(top_n)

"""Kullanıcının Seçmediği, Beğenilen Diğer Kitapları Öner"""

secili_kitaplar = ["God's Smuggler", "Bear Snores On"]
book_ids = kitap_isimden_id_al(secili_kitaplar, books)
kitap_oner_secim_uzerinden(book_ids, filtered_ratings, books)

"""# **Bölüm 9 – SVD Modeli Eğitimi ve Kaydedilmesi (.pkl Dosyası)**"""

!pip install numpy==1.24.3
import os
os.kill(os.getpid(), 9)  # Runtime'ı yeniden başlatır

pip install scikit-surprise

import pandas as pd
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
import joblib

# ratings verisini oku ve aykırı kitapları filtrele
ratings = pd.read_csv("ratings.csv")
book_rating_counts = ratings['book_id'].value_counts()
popular_books = book_rating_counts[book_rating_counts < 5000].index
filtered_ratings = ratings[ratings['book_id'].isin(popular_books)]

# Surprise veri formatı
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(filtered_ratings[
    ['user_id', 'book_id', 'rating']], reader)

# Eğitim-test böl
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# SVD modeli eğit
svd_model = SVD()
svd_model.fit(trainset)

# Kaydet
joblib.dump(svd_model, "svd_model.pkl")
print("✅ SVD modeli başarıyla kaydedildi.")

from surprise import accuracy

# 1. Veriyi ayır
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# 2. Eğit
svd_model = SVD()
svd_model.fit(trainset)

# 3. Test et
predictions = svd_model.test(testset)

# 4. RMSE ve MAE hesapla
print("RMSE:", accuracy.rmse(predictions))
print("MAE :", accuracy.mae(predictions))

dogru_tahmin_sayisi = sum(abs(p.r_ui - p.est) <= 1.0 for p in predictions)
dogruluk_orani = dogru_tahmin_sayisi / len(predictions)
print(f"Doğruluk oranı: {dogruluk_orani:.2%}")

"""# **Bölüm 8 – Streamlit Arayüzü ile Öneri Uygulaması**

Hedefimiz:

Kullanıcının kitap adı seçtiği ve öneri sonuçlarını tablo halinde gördüğü basit ama işlevsel bir arayüz kurmak.

Ne Kullanacağız?

*   streamlit (arayüz)
*   books.csv, filtered_ratings
*   books_with_tags (etiketli kitap bilgisi)

Fonksiyonlarımız:
*   kitap_isimden_id_al()
*   kitap_oner_secim_uzerinden()
"""

pip install streamlit